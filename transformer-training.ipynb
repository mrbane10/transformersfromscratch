{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-22T14:41:53.495573Z",
     "iopub.status.busy": "2025-08-22T14:41:53.494912Z",
     "iopub.status.idle": "2025-08-22T14:41:53.501296Z",
     "shell.execute_reply": "2025-08-22T14:41:53.500572Z",
     "shell.execute_reply.started": "2025-08-22T14:41:53.495546Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T12:22:57.746824Z",
     "iopub.status.busy": "2025-08-22T12:22:57.746570Z",
     "iopub.status.idle": "2025-08-22T12:22:58.499167Z",
     "shell.execute_reply": "2025-08-22T12:22:58.498402Z",
     "shell.execute_reply.started": "2025-08-22T12:22:57.746803Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'transformersfromscratch'...\n",
      "remote: Enumerating objects: 54, done.\u001b[K\n",
      "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
      "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
      "remote: Total 54 (delta 25), reused 41 (delta 15), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (54/54), 17.72 KiB | 4.43 MiB/s, done.\n",
      "Resolving deltas: 100% (25/25), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/mrbane10/transformersfromscratch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T12:23:01.297317Z",
     "iopub.status.busy": "2025-08-22T12:23:01.296970Z",
     "iopub.status.idle": "2025-08-22T12:23:01.303293Z",
     "shell.execute_reply": "2025-08-22T12:23:01.302553Z",
     "shell.execute_reply.started": "2025-08-22T12:23:01.297289Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/transformersfromscratch\n"
     ]
    }
   ],
   "source": [
    "cd /kaggle/working/transformersfromscratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T19:25:40.553157Z",
     "iopub.status.busy": "2025-08-22T19:25:40.552475Z",
     "iopub.status.idle": "2025-08-22T19:25:40.558065Z",
     "shell.execute_reply": "2025-08-22T19:25:40.557307Z",
     "shell.execute_reply.started": "2025-08-22T19:25:40.553136Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from config import get_config\n",
    "import os\n",
    "import wandb\n",
    "cfg = get_config()\n",
    "cfg['model_folder'] = '/kaggle/working/weights'\n",
    "cfg['tokenizer_file'] = '/kaggle/working/tokenizers/tokenizer_{0}.json' \n",
    "cfg['batch_size'] = 16 # Conservative for Kaggle\n",
    "cfg['num_epochs'] = 20\n",
    "cfg['preload'] = \"05\"\n",
    "cfg['seq_len'] = 410\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('/kaggle/working/weights', exist_ok=True)\n",
    "os.makedirs('/kaggle/working/tokenizers', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T14:42:47.676131Z",
     "iopub.status.busy": "2025-08-22T14:42:47.675883Z",
     "iopub.status.idle": "2025-08-22T14:42:50.945905Z",
     "shell.execute_reply": "2025-08-22T14:42:50.945152Z",
     "shell.execute_reply.started": "2025-08-22T14:42:47.676115Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/bin/bash: -c: line 2: syntax error: unexpected end of file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Weights & Biases  (optional)\n",
    "%pip install -q wandb\n",
    "import wandb\n",
    "!wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T14:42:52.644150Z",
     "iopub.status.busy": "2025-08-22T14:42:52.643461Z",
     "iopub.status.idle": "2025-08-22T14:42:54.428057Z",
     "shell.execute_reply": "2025-08-22T14:42:54.427159Z",
     "shell.execute_reply.started": "2025-08-22T14:42:52.644122Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "personal_key_for_api = user_secrets.get_secret(\"wandb_api_key\")\n",
    "!wandb login $personal_key_for_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T14:42:59.577564Z",
     "iopub.status.busy": "2025-08-22T14:42:59.577168Z",
     "iopub.status.idle": "2025-08-22T14:42:59.583658Z",
     "shell.execute_reply": "2025-08-22T14:42:59.582984Z",
     "shell.execute_reply.started": "2025-08-22T14:42:59.577530Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from train import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T14:44:52.742959Z",
     "iopub.status.busy": "2025-08-22T14:44:52.742708Z",
     "iopub.status.idle": "2025-08-22T18:57:16.286038Z",
     "shell.execute_reply": "2025-08-22T18:57:16.285211Z",
     "shell.execute_reply.started": "2025-08-22T14:44:52.742941Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      "Max length of source sentence: 291\n",
      "Max length of target sentence: 406\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▄▅▆▆▆▇▇▇▇█████</td></tr><tr><td>train_loss</td><td>██▆▆▆▅▅▄▄▄▅▄▃▄▄▄▄▃▃▃▃▃▃▂▂▃▂▂▂▃▁▃▃▂▂▁▂▁▁▂</td></tr><tr><td>val/bleu</td><td>▁▁▁▁▁▁</td></tr><tr><td>val/cer</td><td>▃█▂▁▁▃</td></tr><tr><td>val/wer</td><td>▂█▂▁▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>13664</td></tr><tr><td>train_loss</td><td>3.14542</td></tr><tr><td>val/bleu</td><td>0</td></tr><tr><td>val/cer</td><td>0.67521</td></tr><tr><td>val/wer</td><td>1.04</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">solar-capybara-6</strong> at: <a href='https://wandb.ai/ikaisar10-iit-kharagpur/transformer-nmt/runs/qad6dvdv' target=\"_blank\">https://wandb.ai/ikaisar10-iit-kharagpur/transformer-nmt/runs/qad6dvdv</a><br> View project at: <a href='https://wandb.ai/ikaisar10-iit-kharagpur/transformer-nmt' target=\"_blank\">https://wandb.ai/ikaisar10-iit-kharagpur/transformer-nmt</a><br>Synced 5 W&B file(s), 6 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250822_124131-qad6dvdv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/transformersfromscratch/wandb/run-20250822_144501-9lcbqth3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ikaisar10-iit-kharagpur/transformer-nmt/runs/9lcbqth3' target=\"_blank\">balmy-resonance-7</a></strong> to <a href='https://wandb.ai/ikaisar10-iit-kharagpur/transformer-nmt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ikaisar10-iit-kharagpur/transformer-nmt' target=\"_blank\">https://wandb.ai/ikaisar10-iit-kharagpur/transformer-nmt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ikaisar10-iit-kharagpur/transformer-nmt/runs/9lcbqth3' target=\"_blank\">https://wandb.ai/ikaisar10-iit-kharagpur/transformer-nmt/runs/9lcbqth3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading model /kaggle/working/weights/tmodel_05.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 06: 100%|██████████| 1969/1969 [17:57<00:00,  1.83it/s, loss=3.156]\n",
      "/usr/local/lib/python3.11/dist-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `CharErrorRate` from `torchmetrics` was deprecated and will be removed in 2.0. Import `CharErrorRate` from `torchmetrics.text` instead.\n",
      "  _future_warning(\n",
      "/usr/local/lib/python3.11/dist-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `WordErrorRate` from `torchmetrics` was deprecated and will be removed in 2.0. Import `WordErrorRate` from `torchmetrics.text` instead.\n",
      "  _future_warning(\n",
      "Processing epoch 07: 100%|██████████| 1969/1969 [17:59<00:00,  1.82it/s, loss=2.792]\n",
      "Processing epoch 08: 100%|██████████| 1969/1969 [17:59<00:00,  1.82it/s, loss=2.962]\n",
      "Processing epoch 09: 100%|██████████| 1969/1969 [17:59<00:00,  1.82it/s, loss=2.676]\n",
      "Processing epoch 10: 100%|██████████| 1969/1969 [17:59<00:00,  1.82it/s, loss=2.341]\n",
      "Processing epoch 11: 100%|██████████| 1969/1969 [17:57<00:00,  1.83it/s, loss=2.352]\n",
      "Processing epoch 12: 100%|██████████| 1969/1969 [17:57<00:00,  1.83it/s, loss=2.341]\n",
      "Processing epoch 13: 100%|██████████| 1969/1969 [17:57<00:00,  1.83it/s, loss=2.060]\n",
      "Processing epoch 14: 100%|██████████| 1969/1969 [17:59<00:00,  1.82it/s, loss=2.172]\n",
      "Processing epoch 15: 100%|██████████| 1969/1969 [17:59<00:00,  1.82it/s, loss=2.047]\n",
      "Processing epoch 16: 100%|██████████| 1969/1969 [17:58<00:00,  1.83it/s, loss=2.153]\n",
      "Processing epoch 17: 100%|██████████| 1969/1969 [17:58<00:00,  1.82it/s, loss=2.128]\n",
      "Processing epoch 18: 100%|██████████| 1969/1969 [17:58<00:00,  1.83it/s, loss=1.913]\n",
      "Processing epoch 19: 100%|██████████| 1969/1969 [17:58<00:00,  1.83it/s, loss=1.823]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>▁▁▁▁▁▂▂▂▂▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇█</td></tr><tr><td>train_loss</td><td>█▇▆▆▅▆▅▅▅▄▄▄▄▃▄▄▅▄▄▄▃▄▃▃▃▄▃▃▃▂▃▄▃▄▂▂▂▂▁▂</td></tr><tr><td>val/bleu</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/cer</td><td>▇▆▆█▇▁▇▅▆▆▄▄▅▄</td></tr><tr><td>val/wer</td><td>▅▅▅█▅▁▆▄▅▅▃▃▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>39380</td></tr><tr><td>train_loss</td><td>1.82296</td></tr><tr><td>val/bleu</td><td>0</td></tr><tr><td>val/cer</td><td>0.50362</td></tr><tr><td>val/wer</td><td>0.69444</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">balmy-resonance-7</strong> at: <a href='https://wandb.ai/ikaisar10-iit-kharagpur/transformer-nmt/runs/9lcbqth3' target=\"_blank\">https://wandb.ai/ikaisar10-iit-kharagpur/transformer-nmt/runs/9lcbqth3</a><br> View project at: <a href='https://wandb.ai/ikaisar10-iit-kharagpur/transformer-nmt' target=\"_blank\">https://wandb.ai/ikaisar10-iit-kharagpur/transformer-nmt</a><br>Synced 5 W&B file(s), 14 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250822_144501-9lcbqth3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T19:18:04.182584Z",
     "iopub.status.busy": "2025-08-22T19:18:04.182242Z",
     "iopub.status.idle": "2025-08-22T19:18:04.194902Z",
     "shell.execute_reply": "2025-08-22T19:18:04.194306Z",
     "shell.execute_reply.started": "2025-08-22T19:18:04.182557Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from config import get_config, get_weights_file_path\n",
    "from train import get_model, get_ds, run_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T19:25:57.550540Z",
     "iopub.status.busy": "2025-08-22T19:25:57.550272Z",
     "iopub.status.idle": "2025-08-22T19:26:07.194189Z",
     "shell.execute_reply": "2025-08-22T19:26:07.193553Z",
     "shell.execute_reply.started": "2025-08-22T19:25:57.550522Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Max length of source sentence: 291\n",
      "Max length of target sentence: 406\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "model = get_model(cfg, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "\n",
    "# Load the pretrained weights\n",
    "model_filename = get_weights_file_path(config,epoch = 19)\n",
    "state = torch.load(f\"/kaggle/working/{model_filename}\")\n",
    "model.load_state_dict(state['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T19:36:33.116039Z",
     "iopub.status.busy": "2025-08-22T19:36:33.115740Z",
     "iopub.status.idle": "2025-08-22T19:36:36.419472Z",
     "shell.execute_reply": "2025-08-22T19:36:36.418691Z",
     "shell.execute_reply.started": "2025-08-22T19:36:33.116018Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Run validation\n",
    "run_validation(\n",
    "    model, val_dataloader, tokenizer_src, tokenizer_tgt, \n",
    "    config['seq_len'], device, lambda msg: print(msg), \n",
    "    0, None, num_examples=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T19:37:08.976194Z",
     "iopub.status.busy": "2025-08-22T19:37:08.975612Z",
     "iopub.status.idle": "2025-08-22T19:37:08.981395Z",
     "shell.execute_reply": "2025-08-22T19:37:08.980722Z",
     "shell.execute_reply.started": "2025-08-22T19:37:08.976149Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the translate function since it seems to be missing\n",
    "def translate(text, max_len=None):\n",
    "    \"\"\"\n",
    "    Translate a single text using the loaded model\n",
    "    \"\"\"\n",
    "    if max_len is None:\n",
    "        max_len = config['seq_len']\n",
    "    \n",
    "    # Tokenize input text\n",
    "    source_tokens = tokenizer_src.encode(text).ids\n",
    "    \n",
    "    # Add SOS and EOS tokens\n",
    "    sos_idx = tokenizer_src.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_src.token_to_id('[EOS]')\n",
    "    source_tokens = [sos_idx] + source_tokens + [eos_idx]\n",
    "    \n",
    "    # Convert to tensor\n",
    "    source = torch.tensor(source_tokens).unsqueeze(0).to(device)\n",
    "    source_mask = torch.ones(1, 1, len(source_tokens)).to(device)\n",
    "\n",
    " # Use the greedy_decode function from your training script\n",
    "    from train import greedy_decode\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        result = greedy_decode(\n",
    "            model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device\n",
    "        )\n",
    "    \n",
    "    # Decode result\n",
    "    result_tokens = result.detach().cpu().numpy().tolist()\n",
    "    translated_text = tokenizer_tgt.decode(result_tokens)\n",
    "    \n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T20:26:49.634408Z",
     "iopub.status.busy": "2025-08-22T20:26:49.633678Z",
     "iopub.status.idle": "2025-08-22T20:26:50.325981Z",
     "shell.execute_reply": "2025-08-22T20:26:50.325152Z",
     "shell.execute_reply.started": "2025-08-22T20:26:49.634385Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multiple Translation Tests:\n",
      "'Hello, how are you?' -> 'خوش ؟'\n",
      "'What is your name?' -> 'تمہارا نام کیا ہے ؟'\n",
      "'and the listen and obey' -> 'اور کان لگا کر سنا کرو اور ( یوں ) سنا دیں'\n",
      "'This is a great day!' -> 'یہ ایک بہت بڑی آفتوں میں سے ایک دن ہے'\n",
      "'Can you help me with this problem?' -> 'کیا تم مجھ سے کوئی بات میری مدد کر رہے ہو ؟'\n",
      "'and we will test you with difficulties' -> 'اور ہم تمہیں خوب جدا کرکے رہنے دیں گے'\n",
      "'who is the most high' -> 'وہ بلند رتبہ والے ( )'\n",
      "'how has your day been' -> 'آج کے دن کا وقت آگیا ہے'\n"
     ]
    }
   ],
   "source": [
    "# Additional test sentences\n",
    "test_sentences = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"What is your name?\",\n",
    "    \"and the listen and obey\",\n",
    "    \"This is a great day!\",\n",
    "    \"Can you help me with this problem?\",\n",
    "    \"and we will test you with difficulties\",\n",
    "    \"who is the most high\",\n",
    "    \"how has your day been\"\n",
    "]\n",
    "\n",
    "print(f\"\\nMultiple Translation Tests:\")\n",
    "for sentence in test_sentences:\n",
    "    translation = translate(sentence)\n",
    "    print(f\"'{sentence}' -> '{translation}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T19:44:46.653089Z",
     "iopub.status.busy": "2025-08-22T19:44:46.652823Z",
     "iopub.status.idle": "2025-08-22T19:44:46.657583Z",
     "shell.execute_reply": "2025-08-22T19:44:46.656729Z",
     "shell.execute_reply.started": "2025-08-22T19:44:46.653073Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from config import get_config, get_weights_file_path\n",
    "from train import get_model, get_ds, greedy_decode\n",
    "import torchmetrics\n",
    "from torchmetrics.text.bleu import BLEUScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T20:03:53.300457Z",
     "iopub.status.busy": "2025-08-22T20:03:53.300212Z",
     "iopub.status.idle": "2025-08-22T20:03:53.309761Z",
     "shell.execute_reply": "2025-08-22T20:03:53.308945Z",
     "shell.execute_reply.started": "2025-08-22T20:03:53.300441Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def validate_model_detailed(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, num_examples=10):\n",
    "    \"\"\"\n",
    "    Validate the model and display source, expected, and predicted translations\n",
    "    Similar to Umar Jamil's validation format\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    \n",
    "    # For calculating metrics\n",
    "    all_predicted = []\n",
    "    all_expected = []\n",
    "    \n",
    "    # Console formatting\n",
    "    console_width = 120\n",
    "    # print(\"=\"*console_width)\n",
    "    # print(f\"{'VALIDATION RESULTS':^{console_width}}\")\n",
    "    # print(\"=\"*console_width)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in validation_ds:\n",
    "            count += 1\n",
    "            \n",
    "            # Get inputs\n",
    "            encoder_input = batch[\"encoder_input\"].to(device)\n",
    "            encoder_mask = batch[\"encoder_mask\"].to(device)\n",
    "            \n",
    "            # Ensure batch size is 1 for validation\n",
    "            assert encoder_input.size(0) == 1, \"Batch size must be 1 for validation\"\n",
    "            \n",
    "            # Get source and target texts\n",
    "            source_text = batch['src_txt'][0]\n",
    "            target_text = batch['tgt_txt'][0]\n",
    "            \n",
    "            # Generate prediction using greedy decode\n",
    "            model_out = greedy_decode(\n",
    "                model, encoder_input, encoder_mask, \n",
    "                tokenizer_src, tokenizer_tgt, max_len, device\n",
    "            )\n",
    "            \n",
    "            # Decode the prediction\n",
    "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy().tolist())\n",
    "            \n",
    "            # Store for metrics calculation\n",
    "            all_predicted.append(model_out_text)\n",
    "            all_expected.append(target_text)\n",
    "            \n",
    "            # # Display the results\n",
    "            # print(f\"\\nSample {count}:\")\n",
    "            # print(\"-\" * console_width)\n",
    "            \n",
    "            # # Source sentence\n",
    "            # print(f\"SOURCE ({config['lang_src'].upper()}):\")\n",
    "            # print(f\"  {source_text}\")\n",
    "            \n",
    "            # # Expected translation\n",
    "            # print(f\"EXPECTED ({config['lang_tgt'].upper()}):\")\n",
    "            # print(f\"  {target_text}\")\n",
    "            \n",
    "            # # Model prediction\n",
    "            # print(f\"PREDICTED ({config['lang_tgt'].upper()}):\")\n",
    "            # print(f\"  {model_out_text}\")\n",
    "            \n",
    "            # Quick visual comparison (highlight differences)\n",
    "            # print(\"COMPARISON:\")\n",
    "            expected_words = target_text.split()\n",
    "            predicted_words = model_out_text.split()\n",
    "            \n",
    "            # print(f\"  Expected length:  {len(expected_words)} words\")\n",
    "            # print(f\"  Predicted length: {len(predicted_words)} words\")\n",
    "            \n",
    "            if count >= num_examples:\n",
    "                break\n",
    "    \n",
    "    # # Calculate and display metrics\n",
    "    # print(f\"{'OVERALL METRICS':^{console_width}}\")\n",
    "    # print(\"=\"*console_width)\n",
    "    \n",
    "    if len(all_predicted) > 0:\n",
    "        # Character Error Rate\n",
    "        cer_metric = torchmetrics.CharErrorRate()\n",
    "        cer = cer_metric(all_predicted, all_expected).item()\n",
    "        \n",
    "        # Word Error Rate  \n",
    "        wer_metric = torchmetrics.WordErrorRate()\n",
    "        wer = wer_metric(all_predicted, all_expected).item()\n",
    "        \n",
    "        \n",
    "        print(f\"Character Error Rate (CER): {cer:.4f}\")\n",
    "        print(f\"Word Error Rate (WER):      {wer:.4f}\")\n",
    "        print(f\"Number of samples:          {len(all_predicted)}\")\n",
    "    \n",
    "    print(\"=\"*console_width)\n",
    "    \n",
    "    return all_predicted, all_expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T20:03:53.694068Z",
     "iopub.status.busy": "2025-08-22T20:03:53.693825Z",
     "iopub.status.idle": "2025-08-22T20:20:11.387359Z",
     "shell.execute_reply": "2025-08-22T20:20:11.386551Z",
     "shell.execute_reply.started": "2025-08-22T20:03:53.694052Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running detailed validation...\n",
      "Character Error Rate (CER): 0.3588\n",
      "Word Error Rate (WER):      0.5303\n",
      "Number of samples:          3500\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run detailed validation\n",
    "print(f\"\\nRunning detailed validation...\")\n",
    "predictions, expectations = validate_model_detailed(\n",
    "    model, val_dataloader, tokenizer_src, tokenizer_tgt, \n",
    "    config['seq_len'], device, num_examples=len(val_dataloader)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T20:20:19.026965Z",
     "iopub.status.busy": "2025-08-22T20:20:19.026699Z",
     "iopub.status.idle": "2025-08-22T20:20:19.561864Z",
     "shell.execute_reply": "2025-08-22T20:20:19.561218Z",
     "shell.execute_reply.started": "2025-08-22T20:20:19.026945Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected BLEU Score: 0.4391\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "def quick_bleu_fix(predictions, expectations):\n",
    "    smoothing = SmoothingFunction()\n",
    "    \n",
    "    # Clean and prepare data\n",
    "    references = [[exp.split()] for exp in expectations]\n",
    "    candidates = [pred.split() for pred in predictions]\n",
    "    \n",
    "    # Remove empty cases\n",
    "    valid_pairs = [(r, c) for r, c in zip(references, candidates) if len(r[0]) > 0 and len(c) > 0]\n",
    "    \n",
    "    if valid_pairs:\n",
    "        references, candidates = zip(*valid_pairs)\n",
    "        bleu = corpus_bleu(list(references), list(candidates), smoothing_function=smoothing.method1)\n",
    "        return bleu\n",
    "    return 0.0\n",
    "\n",
    "# Test the fix\n",
    "corrected_bleu = quick_bleu_fix(predictions, expectations)\n",
    "print(f\"Corrected BLEU Score: {corrected_bleu:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
